{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install -r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA"
      ],
      "metadata": {
        "id": "VSUf5LWBtllU",
        "outputId": "22a20573-75e2-408a-9f95-d8788a0c73e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "id": "VSUf5LWBtllU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 1)) (1.23.5)\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 2)) (0.15.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (2.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 4)) (1.5.3)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 5)) (2.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (4.35.2)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 2)) (3.20.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 4)) (2023.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (0.19.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (4.66.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (0.41.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (2023.6.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (1.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 7)) (2023.7.22)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->-r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt?token=GHSAT0AAAAAACJI6OXTVIRQEN524EL4R3GIZLDRYUA (line 3)) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0090000b",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "0090000b"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0nuL56xz2uKl"
      },
      "id": "0nuL56xz2uKl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "9C_enaHoSOjn"
      },
      "id": "9C_enaHoSOjn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10395a28",
      "metadata": {
        "id": "10395a28"
      },
      "outputs": [],
      "source": [
        "# BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "BASE_DIR = \"/content/sample_data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2014893a",
      "metadata": {
        "id": "2014893a"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TF_AUTOTUNE\"] = \"1\"\n",
        "tf.keras.backend.set_floatx('float16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5175731e",
      "metadata": {
        "id": "5175731e"
      },
      "outputs": [],
      "source": [
        "# # Enable autotuning\n",
        "# tf.config.threading.set_intra_op_parallelism_threads(1)\n",
        "# tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "# tf.config.threading.set_intra_op_parallelism_threads(1)\n",
        "# tf.config.threading.set_inter_op_parallelism_threads(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b8bd522",
      "metadata": {
        "id": "3b8bd522"
      },
      "outputs": [],
      "source": [
        "train_path = os.path.join(BASE_DIR, 'dataset/train.tsv')\n",
        "test_path = os.path.join(BASE_DIR, 'dataset/test.tsv')\n",
        "validation_path = os.path.join(BASE_DIR, 'dataset/validation.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae948410",
      "metadata": {
        "id": "ae948410"
      },
      "outputs": [],
      "source": [
        "column_labels = ['row', 'json_ids', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state', 'affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'lies_counts', 'context', 'justification']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8a418a8",
      "metadata": {
        "id": "b8a418a8"
      },
      "outputs": [],
      "source": [
        "# Data Frames\n",
        "train = pd.read_csv(train_path, sep=\"\\t\", header=None, names=column_labels)\n",
        "test = pd.read_csv(test_path, sep=\"\\t\", header=None, names=column_labels)\n",
        "valid = pd.read_csv(validation_path, sep=\"\\t\", header=None, names=column_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb3f7eb",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "beb3f7eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd619dba-0959-4f80-d094-051adc41bca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0          false\n",
            "1      half-true\n",
            "2    mostly-true\n",
            "3          false\n",
            "4      half-true\n",
            "5           true\n",
            "6    barely-true\n",
            "7      half-true\n",
            "8      half-true\n",
            "9    mostly-true\n",
            "Name: label, dtype: object\n",
            "0       barely-true\n",
            "1        pants-fire\n",
            "2             false\n",
            "3         half-true\n",
            "4         half-true\n",
            "           ...     \n",
            "1279      half-true\n",
            "1280    mostly-true\n",
            "1281           true\n",
            "1282          false\n",
            "1283    barely-true\n",
            "Name: label, Length: 1284, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Fill nan (empty boxes) with 0\n",
        "train = train.fillna('None')\n",
        "test = test.fillna('None')\n",
        "val = valid.fillna('None')\n",
        "print(train['label'])\n",
        "print(val['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79d60030",
      "metadata": {
        "id": "79d60030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336a6ed0-0943-4ea8-8971-c42ef763b682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'false': 0, 'half-true': 1, 'mostly-true': 2, 'true': 3, 'barely-true': 4, 'pants-fire': 5}\n"
          ]
        }
      ],
      "source": [
        "labels = train['label']\n",
        "label_mapping = {label: idx for idx, label in enumerate(labels.unique())}\n",
        "label_mapping.update({'pants-fire':5})\n",
        "\n",
        "num_of_classes=len(label_mapping)\n",
        "train = train[:100]\n",
        "val = val[:50]\n",
        "# print(num_of_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "594b2c7b",
      "metadata": {
        "id": "594b2c7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d12d2b35-1d13-425e-b6a0-5b533d4ae7fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    0\n",
            "1    1\n",
            "2    2\n",
            "3    0\n",
            "4    1\n",
            "5    3\n",
            "6    4\n",
            "7    1\n",
            "8    1\n",
            "9    2\n",
            "Name: label_encoded, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "train['label_encoded'] = train['label'].map(label_mapping)\n",
        "print(train['label_encoded'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "25df0660",
      "metadata": {
        "id": "25df0660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf5f981-53d3-42c3-8125-df3527d53ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    4\n",
            "1    5\n",
            "2    0\n",
            "3    1\n",
            "4    1\n",
            "5    0\n",
            "6    3\n",
            "7    0\n",
            "8    3\n",
            "9    1\n",
            "Name: label_encoded, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "val['label_encoded'] = val['label'].map(label_mapping)\n",
        "print(val['label_encoded'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_one_hot_labels = to_categorical(train['label_encoded'], num_classes=num_of_classes)\n",
        "# val_one_hot_labels = to_categorical(val['label_encoded'], num_classes=num_of_classes)"
      ],
      "metadata": {
        "id": "1O1YI2HrFoyH"
      },
      "id": "1O1YI2HrFoyH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "6166be7f",
      "metadata": {
        "id": "6166be7f"
      },
      "outputs": [],
      "source": [
        "# model_name = 'experts_wiki_books'\n",
        "# model_name = 'bert-base-uncased'\n",
        "model_name = 'distilbert-base-uncased'\n",
        "# tokenizer = BertTokenizer.from_pretrained(model_name, max_length=128)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name, max_length=128)\n",
        "vocab_size = 10000\n",
        "embedding_dim = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a7bc0c",
      "metadata": {
        "id": "11a7bc0c"
      },
      "outputs": [],
      "source": [
        "# Create a custom embedding layer\n",
        "# custom_embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7c84152",
      "metadata": {
        "id": "a7c84152"
      },
      "outputs": [],
      "source": [
        "# Only using statement data at first\n",
        "# Tokenize the statement data\n",
        "train_encoded_statement_data = tokenizer(\n",
        "    train['statement'].to_list(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "print(train_encoded_statement_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1628a3a2",
      "metadata": {
        "id": "1628a3a2"
      },
      "source": [
        "train_labels = train['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "ffca59d5",
      "metadata": {
        "id": "ffca59d5"
      },
      "outputs": [],
      "source": [
        "val_encoded_statement_data = tokenizer(\n",
        "    val['statement'].tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors='tf'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2053b268",
      "metadata": {
        "id": "2053b268"
      },
      "source": [
        "val_labels = val['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "8dca3abc",
      "metadata": {
        "id": "8dca3abc"
      },
      "outputs": [],
      "source": [
        "# Create TensorFlow dataset for training\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': train_encoded_statement_data['input_ids'],\n",
        "        'attention_mask': train_encoded_statement_data['attention_mask']\n",
        "    },\n",
        "    train['label_encoded'] ))  # using one-hot encoded labels when CategoricalCrossEntropy used,\n",
        "                            # and when using SparseCrossEntropy use train['label_encoded'] which is int rep for labels : 0, 1, 2 ..5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "3141466a",
      "metadata": {
        "id": "3141466a"
      },
      "outputs": [],
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': val_encoded_statement_data['input_ids'],\n",
        "        'attention_mask': val_encoded_statement_data['attention_mask']\n",
        "    },\n",
        "    val['label_encoded'] ))  # using one-hot encoded labels when CategoricalCrossEntropy used,\n",
        "                            # and when using SparseCrossEntropy use train['label_encoded'] which is int rep for labels : 0, 1, 2 ..5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "e384fa59",
      "metadata": {
        "id": "e384fa59"
      },
      "outputs": [],
      "source": [
        "# Limiting the dataset\n",
        "limit = 10\n",
        "limited_train_dataset = train_dataset.take(limit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "e9bdd426",
      "metadata": {
        "id": "e9bdd426",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d911432-292d-4e3f-9638-dbb2925bb86f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "b518ae07",
      "metadata": {
        "id": "b518ae07"
      },
      "outputs": [],
      "source": [
        "num_epochs = 1\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "35146986",
      "metadata": {
        "id": "35146986"
      },
      "outputs": [],
      "source": [
        "# # Create a new model with the BERT base and the custom output layer\n",
        "\n",
        "input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='attention_mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b836c7d",
      "metadata": {
        "id": "2b836c7d"
      },
      "source": [
        "Custom embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b752035",
      "metadata": {
        "id": "3b752035"
      },
      "source": [
        "custom_embeddings = custom_embedding_layer(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "e74afa49",
      "metadata": {
        "id": "e74afa49"
      },
      "outputs": [],
      "source": [
        "# Adding a dense layer for the output\n",
        "dense_layer = tf.keras.layers.Dense(num_of_classes, activation='softmax', name='dense_output')\n",
        "bert_output = model([input_ids, attention_mask])\n",
        "cls_token = bert_output.logits\n",
        "dense_output = dense_layer(cls_token)\n",
        "# dense_output_expanded = tf.keras.layers.Reshape((1, 6))(dense_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "e98178aa",
      "metadata": {
        "id": "e98178aa"
      },
      "outputs": [],
      "source": [
        "# combined_embeddings = tf.keras.layers.Concatenate(axis=-1)([dense_output_expanded, custom_embeddings])\n",
        "output = tf.keras.layers.Dense(num_of_classes, activation='softmax')(dense_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "5de23a06",
      "metadata": {
        "id": "5de23a06"
      },
      "outputs": [],
      "source": [
        "# Create the final model\n",
        "custom_model = tf.keras.Model(inputs=model.input, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "c2cb8151",
      "metadata": {
        "id": "c2cb8151"
      },
      "outputs": [],
      "source": [
        "custom_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(),        #BinaryCrossEntropy for binary classification; for now lets only classify acc to data: 6 classes\n",
        "            metrics=tf.keras.metrics.SparseCategoricalAccuracy())  # or use ['accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "6f807bea",
      "metadata": {
        "id": "6f807bea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f848e63-56a2-419e-d8c0-e5ab738805b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)      [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf_distil_bert_for_sequenc  TFSequenceClassifierOutput   6695501   ['input_ids[0][0]',           \n",
            " e_classification_1 (TFDist  (loss=None, logits=(None,    0          'attention_mask[0][0]']      \n",
            " ilBertForSequenceClassific  2),                                                                  \n",
            " ation)                       hidden_states=None, atten                                           \n",
            "                             tions=None)                                                          \n",
            "                                                                                                  \n",
            " dense_output (Dense)        (None, 7)                    21        ['tf_distil_bert_for_sequence_\n",
            "                                                                    classification_1[1][0]']      \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 7)                    56        ['dense_output[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66955087 (127.71 MB)\n",
            "Trainable params: 66955087 (127.71 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "custom_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "12c663b6",
      "metadata": {
        "id": "12c663b6"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(BASE_DIR, 'model_checkpoint'),  # Specify the path to save the checkpoint\n",
        "    save_best_only=True,  # Save only the best model based on the validation loss\n",
        "    monitor='val_loss',  # Monitor the validation loss\n",
        "    mode='min',  # Mode can be 'min' or 'max' depending on the monitored metric\n",
        "    verbose=1  # Show progress while saving\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "train_steps_per_epoch = len(limited_train_dataset)\n",
        "tqdm_callback = tf.keras.callbacks.LambdaCallback(\n",
        "    on_epoch_begin=lambda epoch, logs: tqdm(total=train_steps_per_epoch, position=0, desc=\"Epoch\", unit=\"batch\"),\n",
        "    on_epoch_end=lambda epoch, logs: tqdm.write(f'Epoch {epoch + 1}/{num_epochs}, Loss: {logs[\"loss\"]}, Accuracy: {logs[\"sparse_categorical_accuracy\"]}, Val Loss: {logs[\"val_loss\"]}, Val Accuracy: {logs[\"val_sparse_categorical_accuracy\"]}'),\n",
        "    on_batch_end=lambda batch, logs: tqdm.update(1)\n",
        ")\n",
        "\n",
        "class ProgressBarCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        self.epochs += 1\n",
        "        self.pbar.update(1)\n",
        "        self.pbar.set_postfix(logs, refresh=True)\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.epochs = 0\n",
        "        self.pbar = tqdm(total=self.params['epochs'], unit='epoch', position=0)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        self.pbar.close()"
      ],
      "metadata": {
        "id": "G_Yzo60DERzH"
      },
      "id": "G_Yzo60DERzH",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "ab08e7ac",
      "metadata": {
        "id": "ab08e7ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9793c33-622f-4dcd-a5fe-b45568b6f7dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [26:08<?, ?epoch/s]\n",
            "  0%|          | 0/1 [39:47<?, ?epoch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 2.15234, saving model to /content/sample_data/model_checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x79a46aec8280>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x79a46aeca9b0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x79a46ad15510>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x79a46ad17fd0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x79a46ad02bc0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x79a46adf1750>, because it is not built.\n",
            "100%|██████████| 1/1 [08:29<00:00, 509.38s/epoch, loss=1.98, sparse_categorical_accuracy=0, val_loss=2.15, val_sparse_categorical_accuracy=0]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 - 509s - loss: 1.9844 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 2.1523 - val_sparse_categorical_accuracy: 0.0000e+00 - 509s/epoch - 509s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 1/1 [08:29<00:00, 509.40s/epoch, loss=1.98, sparse_categorical_accuracy=0, val_loss=2.15, val_sparse_categorical_accuracy=0]\n"
          ]
        }
      ],
      "source": [
        "print('Start training')\n",
        "progress_bar_callback = ProgressBarCallback()\n",
        "history = custom_model.fit(\n",
        "    limited_train_dataset.shuffle(10).batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
        "    epochs=num_epochs,\n",
        "    validation_data=val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
        "    verbose=2,\n",
        "    callbacks=[checkpoint_callback, progress_bar_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "27d5ed1c",
      "metadata": {
        "id": "27d5ed1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "35871c41-690c-48e5-e27d-85db9a63eb11"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-80c6c969df23>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the trained model if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcustom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trained_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'custom_model' is not defined"
          ]
        }
      ],
      "source": [
        "# Save the trained model if needed\n",
        "custom_model.save(os.path.join(BASE_DIR, 'trained_model'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import Progbar\n",
        "\n",
        "# Tokenize and preprocess the test data\n",
        "test_encoded_statement_data = tokenizer(\n",
        "    test['statement'].tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "test['label_encoded'] = train['label'].map(label_mapping)\n",
        "\n",
        "# Create TensorFlow dataset for testing\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': test_encoded_statement_data['input_ids'],\n",
        "        'attention_mask': test_encoded_statement_data['attention_mask']\n",
        "    },\n",
        "    test['label_encoded']\n",
        "))\n",
        "limited_test_dataset = test_dataset.take(limit)\n",
        "test_steps = len(limited_test_dataset)\n",
        "progbar = Progbar(test_steps)\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "results = custom_model.evaluate(limited_test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "# Display the results and update the progress bar\n",
        "for metric_name, result in zip(custom_model.metrics_names, results):\n",
        "    print(f'{metric_name}: {result}')\n",
        "\n",
        "    # Update progress bar\n",
        "    progbar.update(1)\n",
        "# Print the evaluation results (including accuracy)\n",
        "print(\"Test Loss:\", results[0])\n",
        "print(\"Test Accuracy:\", results[1])"
      ],
      "metadata": {
        "id": "aliO9ePe7IqA"
      },
      "id": "aliO9ePe7IqA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}