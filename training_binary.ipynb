{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFFBh28xlJOA"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/LizaAmatya/FakeNewsDetection/main/requirements.txt"
      ],
      "id": "BFFBh28xlJOA"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8ea082eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "982b7230-532a-4430-c11b-40b45993a872"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fb321ab3540a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFDistilBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np"
      ],
      "id": "8ea082eb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVXZhU2-p-1Z"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.run_functions_eagerly(False)\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "# mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\",\"/gpu:2\", \"/gpu:3\"])"
      ],
      "id": "NVXZhU2-p-1Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33b66a03"
      },
      "outputs": [],
      "source": [
        "# BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "BASE_DIR = \"/content/sample_data\""
      ],
      "id": "33b66a03"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a2a53ae"
      },
      "outputs": [],
      "source": [
        "# Enable autotuning\n",
        "# tf.config.threading.set_intra_op_parallelism_threads(1)\n",
        "# tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "# tf.config.threading.set_intra_op_parallelism_threads(1)\n",
        "# tf.config.threading.set_inter_op_parallelism_threads(1)"
      ],
      "id": "7a2a53ae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a3f59c0"
      },
      "outputs": [],
      "source": [
        "train_path = os.path.join(BASE_DIR, 'dataset/train.tsv')\n",
        "test_path = os.path.join(BASE_DIR, 'dataset/test.tsv')\n",
        "validation_path = os.path.join(BASE_DIR, 'dataset/validation.tsv')"
      ],
      "id": "7a3f59c0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f59efd84"
      },
      "outputs": [],
      "source": [
        "column_labels = ['row', 'json_ids', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state', 'affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'lies_counts', 'context', 'justification']"
      ],
      "id": "f59efd84"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1c87785"
      },
      "outputs": [],
      "source": [
        "# Data Frames\n",
        "train = pd.read_csv(train_path, sep=\"\\t\", header=None, names=column_labels)\n",
        "test = pd.read_csv(test_path, sep=\"\\t\", header=None, names=column_labels)\n",
        "valid = pd.read_csv(validation_path, sep=\"\\t\", header=None, names=column_labels)"
      ],
      "id": "c1c87785"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aab57d9f"
      },
      "outputs": [],
      "source": [
        "# Fill nan (empty boxes) with 0\n",
        "train = train.fillna('None')\n",
        "test = test.fillna('None')\n",
        "val = valid.fillna('None')"
      ],
      "id": "aab57d9f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94efce03"
      },
      "source": [
        "Mapping to binary classes"
      ],
      "id": "94efce03"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85661522"
      },
      "outputs": [],
      "source": [
        "labels = train['label']\n",
        "label_mapping = {label: idx for idx, label in enumerate(labels.unique())}\n",
        "print('label mapping', label_mapping)"
      ],
      "id": "85661522"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac771165"
      },
      "outputs": [],
      "source": [
        "# Converting into 2 labels\n",
        "train['label_encoded'] = np.where(np.isin(labels, ['mostly-true', 'true']), 1, 0)   # Mapping True as 1 and lies as 0\n",
        "val['label_encoded'] = np.where(np.isin(val['label'], ['mostly-true', 'true']), 1, 0)"
      ],
      "id": "ac771165"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35c3a7fb"
      },
      "outputs": [],
      "source": [
        "num_of_classes=2"
      ],
      "id": "35c3a7fb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b232da14"
      },
      "outputs": [],
      "source": [
        "# model_name = 'experts_wiki_books'\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)"
      ],
      "id": "b232da14"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "737402ea"
      },
      "outputs": [],
      "source": [
        "# Tokenize the statement and metadata\n",
        "train_encoded_statement_data = tokenizer(\n",
        "    train['statement'].to_list(),\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "train['metadata'] = train['subject'].astype(str) + ' ' + train['speaker'].astype(str) + ' ' + train['job_title'].astype(str) + ' ' + train['state'].astype(str) + ' ' + train['affiliation'].astype(str) + ' ' + train['context'].astype(str)\n",
        "\n",
        "train_encoded_metadata = tokenizer(\n",
        "    train['metadata'].to_list(),\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")"
      ],
      "id": "737402ea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec6dca67"
      },
      "outputs": [],
      "source": [
        "val_encoded_statement_data = tokenizer(\n",
        "    val['statement'].tolist(),\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "val['metadata'] = val['subject'].astype(str) + ' ' + val['speaker'].astype(str) + ' ' + val['job_title'].astype(str) + ' ' + val['state'].astype(str) + ' ' + val['affiliation'].astype(str) + ' ' + val['context'].astype(str)\n",
        "\n",
        "val_encoded_metadata = tokenizer(\n",
        "    val['metadata'].to_list(),\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")"
      ],
      "id": "ec6dca67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5347cba"
      },
      "outputs": [],
      "source": [
        "# Create TensorFlow dataset for training\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': train_encoded_statement_data['input_ids'],\n",
        "        'attention_mask': train_encoded_statement_data['attention_mask'],\n",
        "        'input_ids_metadata': train_encoded_metadata['input_ids'],\n",
        "        'attention_mask_metadata': train_encoded_metadata['attention_mask']\n",
        "    },\n",
        "    train['label_encoded'] ))"
      ],
      "id": "d5347cba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bb990b1"
      },
      "outputs": [],
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': val_encoded_statement_data['input_ids'],\n",
        "        'attention_mask': val_encoded_statement_data['attention_mask'],\n",
        "        'input_ids_metadata': val_encoded_metadata['input_ids'],\n",
        "        'attention_mask_metadata': val_encoded_metadata['attention_mask']\n",
        "\n",
        "    },\n",
        "    val['label_encoded'] ))  # using one-hot encoded labels when CategoricalCrossEntropy used,\n",
        "                            # and when using SparseCrossEntropy use train['label_encoded'] which is int rep for labels : 0, 1, 2 ..5"
      ],
      "id": "3bb990b1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfa1b659"
      },
      "outputs": [],
      "source": [
        "# Limiting the dataset\n",
        "# limit = 50\n",
        "# limited_train_dataset = train_dataset.take(limit)"
      ],
      "id": "dfa1b659"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9df7d801"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "batch_size = 32"
      ],
      "id": "9df7d801"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlfQDU3Jp12_"
      },
      "outputs": [],
      "source": [
        "# Creating model with BERT\n",
        "from tensorflow.keras.layers import Input\n",
        "max_length = 128\n",
        "def create_bert_model():\n",
        "    model_name = 'bert-base-uncased'\n",
        "    model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "    # Define input layers\n",
        "    input_ids_statement = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask_statement = Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
        "    input_ids_metadata = Input(shape=(max_length,), dtype=tf.int32, name='input_ids_metadata')\n",
        "    attention_mask_metadata = Input(shape=(max_length,), dtype=tf.int32, name='attention_mask_metadata')\n",
        "\n",
        "    bert_output_statement = model([input_ids_statement, attention_mask_statement])\n",
        "    bert_output_metadata = model([input_ids_metadata, attention_mask_metadata])\n",
        "\n",
        "    # Extract logits for the positive class from each branch\n",
        "    positive_class_statement_logits = bert_output_statement.logits[:, 1]\n",
        "    positive_class_metadata_logits = bert_output_metadata.logits[:, 1]\n",
        "\n",
        "    positive_class_statement_logits = tf.expand_dims(positive_class_statement_logits, axis=-1)\n",
        "    positive_class_metadata_logits = tf.expand_dims(positive_class_metadata_logits, axis=-1)\n",
        "\n",
        "    # Add dense layers for classification on each branch\n",
        "    dense_layer_statement = tf.keras.layers.Dense(256, activation='relu')(positive_class_statement_logits)\n",
        "    dense_layer_metadata = tf.keras.layers.Dense(256, activation='relu')(bert_output_metadata.logits)\n",
        "\n",
        "    # Merge the outputs using Concatenate or other merging strategies\n",
        "    merged_output = tf.keras.layers.Concatenate(axis=-1)([dense_layer_statement, dense_layer_metadata])\n",
        "\n",
        "    # Add additional dense layers for classification\n",
        "    final_dense_layer = tf.keras.layers.Dense(128, activation='relu')(merged_output)\n",
        "    output = tf.keras.layers.Dense(1, activation='sigmoid')(final_dense_layer)\n",
        "\n",
        "    custom_model = tf.keras.Model(inputs=[input_ids_statement, attention_mask_statement, input_ids_metadata, attention_mask_metadata],\n",
        "                                  outputs=output)\n",
        "    custom_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "    return custom_model"
      ],
      "id": "ZlfQDU3Jp12_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed5ec3d8"
      },
      "outputs": [],
      "source": [
        "# Create a new model with the BERT base and the custom output layer\n",
        "\n",
        "def create_model():\n",
        "  model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
        "  input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_ids')\n",
        "  attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "  # Adding a dense layer for the output\n",
        "  bert_output = model([input_ids, attention_mask])\n",
        "  cls_token = bert_output.logits\n",
        "  positive_class_logits = cls_token[:, 1]  # Extract logits for the positive class\n",
        "  positive_class_logits = tf.expand_dims(positive_class_logits, axis=-1)\n",
        "  dense_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='dense_output')\n",
        "  output = dense_layer(positive_class_logits)\n",
        "\n",
        "  # dense layer with 1 unit and use an appropriate activation function (such as sigmoid) in a binary classification scenario,\n",
        "  # it means that the output of the model will be a single scalar value between 0 and 1. This scalar can be interpreted as the predicted probability of belonging to the positive class.\n",
        "  # Making logits and labels dim same\n",
        "  output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\n",
        "  custom_model = tf.keras.Model(inputs=model.input, outputs=output)\n",
        "  custom_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "  return custom_model"
      ],
      "id": "ed5ec3d8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18653174"
      },
      "outputs": [],
      "source": [
        "# strategy = tf.distribute.MirroredStrategy()\n",
        "with tpu_strategy.scope():\n",
        "    custom_model = create_bert_model()\n",
        "\n",
        "custom_model.summary()"
      ],
      "id": "18653174"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhq2LVRQs3xI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(custom_model, to_file='binary_custom_model.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# If you want to save separate images for train, validation, and test datasets\n",
        "plot_model(custom_model, to_file='train_model_binary.png', show_shapes=True, show_layer_names=True, rankdir='LR')\n",
        "# plot_model(custom_model, to_file='validation_model.png', show_shapes=True, show_layer_names=True, rankdir='LR')\n",
        "# plot_model(custom_model, to_file='test_model.png', show_shapes=True, show_layer_names=True, rankdir='LR')"
      ],
      "id": "Vhq2LVRQs3xI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b27642ef"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(BASE_DIR, 'model_checkpoint'),  # Specify the path to save the checkpoint\n",
        "    save_best_only=True,  # Save only the best model based on the validation loss\n",
        "    monitor='val_loss',  # Monitor the validation loss\n",
        "    mode='min',  # Mode can be 'min' or 'max' depending on the monitored metric\n",
        "    verbose=1  # Show progress while saving\n",
        ")"
      ],
      "id": "b27642ef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpUOukvfoKvL"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "train_steps_per_epoch = len(train_dataset)\n",
        "tqdm_callback = tf.keras.callbacks.LambdaCallback(\n",
        "    on_epoch_begin=lambda epoch, logs: tqdm(total=train_steps_per_epoch, position=0, desc=\"Epoch\", unit=\"batch\"),\n",
        "    on_epoch_end=lambda epoch, logs: tqdm.write(f'Epoch {epoch + 1}/{num_epochs}, Loss: {logs[\"loss\"]}, Accuracy: {logs[\"sparse_categorical_accuracy\"]}, Val Loss: {logs[\"val_loss\"]}, Val Accuracy: {logs[\"val_sparse_categorical_accuracy\"]}'),\n",
        "    on_batch_end=lambda batch, logs: tqdm.update(1)\n",
        ")\n",
        "\n",
        "class ProgressBarCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        self.epochs += 1\n",
        "        self.pbar.update(1)\n",
        "        self.pbar.set_postfix(logs, refresh=True)\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.epochs = 0\n",
        "        self.pbar = tqdm(total=self.params['epochs'], unit='epoch', position=0)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        self.pbar.close()"
      ],
      "id": "ZpUOukvfoKvL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d10df90"
      },
      "outputs": [],
      "source": [
        "print('Start training')\n",
        "progress_bar_callback = ProgressBarCallback()\n",
        "history = custom_model.fit(\n",
        "    train_dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
        "    epochs=num_epochs,\n",
        "    validation_data=val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
        "    verbose=1,\n",
        "    callbacks=[progress_bar_callback]\n",
        ")"
      ],
      "id": "6d10df90"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "011fee0a"
      },
      "outputs": [],
      "source": [
        "# Save the trained model if needed\n",
        "# custom_model.save(os.path.join(BASE_DIR, 'trained_model'))"
      ],
      "id": "011fee0a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctsV-8accvYm"
      },
      "outputs": [],
      "source": [
        "# from keras.utils import Progbar\n",
        "\n",
        "# Tokenize and preprocess the test data\n",
        "test_encoded_statement_data = tokenizer(\n",
        "    test['statement'].tolist(),\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "test['label_encoded'] = train['label'].map(label_mapping)\n",
        "test['metadata'] = test['subject'].astype(str) + ' ' + test['speaker'].astype(str) + ' ' + test['job_title'].astype(str) + ' ' + test['state'].astype(str) + ' ' + test['affiliation'].astype(str) + ' ' + test['context'].astype(str)\n",
        "\n",
        "test_encoded_metadata = tokenizer(\n",
        "    test['metadata'].to_list(),\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "# Create TensorFlow dataset for testing\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': test_encoded_statement_data['input_ids'],\n",
        "        'attention_mask': test_encoded_statement_data['attention_mask'],\n",
        "        'input_ids_metadata': test_encoded_metadata['input_ids'],\n",
        "        'attention_mask_metadata': test_encoded_metadata['attention_mask']\n",
        "    },\n",
        "    test['label_encoded']\n",
        "))\n",
        "\n",
        "test_steps = len(test_dataset)\n",
        "# progbar = Progbar(test_steps)\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "with tpu_strategy.scope():\n",
        "  results = custom_model.evaluate(test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "# Display the results and update the progress bar\n",
        "for metric_name, result in zip(custom_model.metrics_names, results):\n",
        "    print(f'{metric_name}: {result}')\n",
        "\n",
        "    # Update progress bar\n",
        "    # progbar.update(1)\n",
        "# Print the evaluation results (including accuracy)\n",
        "print(\"Test Loss:\", results[0])\n",
        "print(\"Test Accuracy:\", results[1])"
      ],
      "id": "ctsV-8accvYm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCfWLaaDm5Nw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "id": "uCfWLaaDm5Nw"
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}